Image Classification Using CNN
Let's see a working example of training a convolutional neural network (CNN) on a dataset of flower images.

Steps to be followed:
Import the necessary libraries and dataset
Count and retrieve the images
Create a training dataset
Create a validation dataset
Visualize a subset of images from the training dataset
Preprocess and normalize the training dataset
Create a convolutional neural network model with data augmentation
Summarize and compile the model
Train the model
Visualize the result
Predict the class of a given image

Step 1: Import the necessary libraries and dataset
Import the required libraries.
Import the given dataset using the link provided.
1
import matplotlib.pyplot as plt
2
import numpy as np
3
import os
4
import PIL
5
import tensorflow as tf
6
​
7
from tensorflow import keras
8
from tensorflow.keras import layers
9
from tensorflow.keras.models import Sequential
10
​
1
import pathlib
2
dataset_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"
3
data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)
4
data_dir = pathlib.Path(data_dir)
5
​
Downloading data from https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz
228813984/228813984 [==============================] - 8s 0us/step

Step 2: Count and retrieve the images
Count the number of images in the directory specified by data_dir and print the count.
Retrieve the file paths of the images in the roses subdirectory and display the first two images.
Retrieve the file paths of the images in the tulips subdirectory and display the first image.
1
image_count = len(list(data_dir.glob('*/*.jpg')))
2
print(image_count)
1
roses = list(data_dir.glob('roses/*'))
2
PIL.Image.open(str(roses[0]))
1
PIL.Image.open(str(roses[1]))
1
tulips = list(data_dir.glob('tulips/*'))
2
PIL.Image.open(str(tulips[0]))

Step 3: Create a training dataset
Set the batch size, image height, and image width variables.
Create a training dataset using tf.keras.utils.image_dataset_from_directory() function, passing the following parameters:
data_dir: The directory containing the image dataset
validation_split: The fraction of data to reserve for validation
subset: Specify the subset of the dataset to use (in this case, training)
seed: Random seed for shuffling the data
image_size: The desired size for the images in the dataset
batch_size: The number of samples per batch
1
batch_size = 32
2
img_height = 180
3
img_width = 180
4
​
5
​
6
train_ds = tf.keras.utils.image_dataset_from_directory(
7
  data_dir,
8
  validation_split=0.2,
9
  subset="training",
10
  seed=123,
11
  image_size=(img_height, img_width),
12
  batch_size=batch_size)
Observation:

The train_ds object represents the training dataset.

Step 4: Create a validation dataset
Create a validation dataset using tf.keras.utils.image_dataset_from_directory() function, passing the following parameters:
data_dir: The directory containing the image dataset
validation_split: The fraction of data to reserve for validation
subset: Specify the subset of the dataset to use (in this case, validation)
seed: Random seed for shuffling the data
image_size: The desired size for the images in the dataset
batch_size: The number of samples per batch
1
val_ds = tf.keras.utils.image_dataset_from_directory(
2
  data_dir,
3
  validation_split=0.2,
4
  subset="validation",
5
  seed=123,
6
  image_size=(img_height, img_width),
7
  batch_size=batch_size)
Observation:

The val_ds object represents the validation dataset.

Step 5: Visualize a subset of images from the training dataset
Obtain the class names from the train_ds dataset using the class_names attribute.
Print the class_names to display the list of class labels.
Import the matplotlib.pyplot module for visualization purposes.
Create a figure with a size of 10x10 using plt.figure(figsize=(10, 10)).
Iterate over the first batch of images and labels in the train_ds dataset using train_ds.take(1).
For each image in the batch (up to 9 images), create a subplot using plt.subplot(3, 3, i + 1).
Display the image using plt.imshow(images[i].numpy().astype("uint8")).
Set the title of the subplot to the corresponding class name using plt.title(class_names[labels[i]]).
Disable the axis labels for the subplot using plt.axis("off").
1
class_names = train_ds.class_names
2
print(class_names)
1
import matplotlib.pyplot as plt
2
​
3
plt.figure(figsize=(10, 10))
4
for images, labels in train_ds.take(1):
5
  for i in range(9):
6
    ax = plt.subplot(3, 3, i + 1)
7
    plt.imshow(images[i].numpy().astype("uint8"))
8
    plt.title(class_names[labels[i]])
9
    plt.axis("off")
Observation:

The grid of nine subplots provides a visual representation of images from the training dataset and their respective class labels. This arrangement allows for easy comparison and analysis of the various image classes within the dataset.

Step 6: Preprocess and normalize the training dataset
Set the value of AUTOTUNE to tf.data.AUTOTUNE.
Cache the train_ds dataset for improved performance by calling the cache() method.
Shuffle the elements of the train_ds dataset using a buffer size of 1000 by calling the shuffle() method.
Prefetch the elements of the train_ds dataset for improved performance by calling the prefetch() method with buffer_size=AUTOTUNE.
Cache the val_ds dataset for improved performance by calling the cache() method.
Prefetch the elements of the val_ds dataset for improved performance by calling the prefetch() method with buffer_size=AUTOTUNE.
Create a Rescaling layer to normalize the pixel values of the dataset images to the range [0, 1].
Apply the normalization_layer to the train_ds dataset using the map() method and lambda function.
Retrieve a batch of images and labels from the normalized dataset using next**(iter(normalized_ds)).**
Access the first image in the batch using image_batch[0].
Print the minimum and maximum pixel values of the first image using np.min(first_image) and np.max(first_image).
1
AUTOTUNE = tf.data.AUTOTUNE
2
​
3
train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
4
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)
1
normalization_layer = layers.Rescaling(1./255)
2
​
3
normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))
4
image_batch, labels_batch = next(iter(normalized_ds))
5
first_image = image_batch[0]
6
print(np.min(first_image), np.max(first_image))
7
​
Observation:

The range of pixel values in the normalized dataset ensures that the minimum value is 0 and the maximum value is 1 for the first image. This normalization process allows for consistent and standardized pixel values, facilitating easier comparisons and computations in subsequent analysis.

Step 7: Create a convolutional neural network model with data augmentation
Create a data augmentation pipeline using keras.Sequential with three augmentation layers: random horizontal flip, random rotation, and random zoom.
Determine the number of classes based on the class_names.
1
data_augmentation = keras.Sequential(
2
  [
3
    layers.RandomFlip("horizontal",
4
                      input_shape=(img_height,
5
                                  img_width,
6
                                  3)),
7
    layers.RandomRotation(0.1),
8
    layers.RandomZoom(0.1),
9
  ]
10
)
Create a sequential model with data augmentation as the first layer and rescaling layer.
Add convolutional layers with an increasing number of filters, 3x3 kernel, padding, and ReLU activation.
Add max pooling layers after each convolutional layer.
Add a dropout layer with a rate of 0.2.
Add dense layers with ReLU activation, ending with a dense output layer.
1
num_classes = len(class_names)
2
model = Sequential([
3
  data_augmentation,
4
  layers.Rescaling(1./255),
5
  layers.Conv2D(16, 3, padding='same', activation='relu'),
6
  layers.MaxPooling2D(),
7
  layers.Conv2D(32, 3, padding='same', activation='relu'),
8
  layers.MaxPooling2D(),
9
  layers.Conv2D(64, 3, padding='same', activation='relu'),
10
  layers.MaxPooling2D(),
11
  layers.Dropout(0.2),
12
  layers.Flatten(),
13
  layers.Dense(128, activation='relu'),
14
  layers.Dense(num_classes)
15
])
16
​
Observation:

The output of this code is a convolutional neural network model with data augmentation, suitable for image classification tasks.

Step 8: Summarize and compile the model
Display the summary of the model architecture and the number of parameters.
1
model.summary()
Compile the model with the specified optimizer, loss function, and metrics.
1
model.compile(optimizer='adam',
2
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
3
              metrics=['accuracy'])
4
​
5
​
Observation:

The summary of the model architecture and the compiled model can be seen as output.

Step 9: Train the model
Set the number of epochs to 3
Train the model using the fit method and pass the training and validation datasets and the number of epochs
1
epochs=3
2
history = model.fit(
3
  train_ds,
4
  validation_data=val_ds,
5
  epochs=epochs
6
)
Observation:

The training history object provides valuable information about the training process, including the loss and accuracy values for each epoch. This object allows us to track the performance of the model over time and analyze how the loss and accuracy metrics evolve during training.

Step 10: Visualize the result
Retrieve the accuracy and loss values from the training history.
Create a range of epochs.
Plot the training and validation accuracy in a subplot.
Plot the training and validation loss in a subplot.
Display the plotted figures.
1
acc = history.history['accuracy']
2
val_acc = history.history['val_accuracy']
3
​
4
loss = history.history['loss']
5
val_loss = history.history['val_loss']
6
​
7
epochs_range = range(epochs)
8
​
9
plt.figure(figsize=(8, 8))
10
plt.subplot(1, 2, 1)
11
plt.plot(epochs_range, acc, label='Training Accuracy')
12
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
13
plt.legend(loc='lower right')
14
plt.title('Training and Validation Accuracy')
15
​
16
plt.subplot(1, 2, 2)
17
plt.plot(epochs_range, loss, label='Training Loss')
18
plt.plot(epochs_range, val_loss, label='Validation Loss')
19
plt.legend(loc='upper right')
20
plt.title('Training and Validation Loss')
21
plt.show()
Observation:

Two subplots show the training and validation accuracy and the training and validation loss over the range of epochs.

Step 11: Predict the class of a given image
Define the URL of the image and download it using tf.keras.utils.get_file().
Load the image and resize it to the desired target size.
Convert the image to an array and expand its dimensions to create a batch.
Make predictions on the image using the trained model.
Calculate the softmax scores and identify the class with the highest confidence.
Print the predicted class and its corresponding confidence percentage.

sunflower_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/592px-Red_sunflower.jpg"
sunflower_path = tf.keras.utils.get_file('Red_sunflower', origin=sunflower_url)

img = tf.keras.utils.load_img(
    sunflower_path, target_size=(img_height, img_width)
)
img_array = tf.keras.utils.img_to_array(img)
img_array = tf.expand_dims(img_array, 0)

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])

print(
    "This image most likely belongs to {} with a {:.2f} percent confidence."
    .format(class_names[np.argmax(score)], 100 * np.max(score))
)
1
​
2
sunflower_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/592px-Red_sunflower.jpg"
3
sunflower_path = tf.keras.utils.get_file('Red_sunflower', origin=sunflower_url)
4
​
5
img = tf.keras.utils.load_img(
6
    sunflower_path, target_size=(img_height, img_width)
7
)
8
img_array = tf.keras.utils.img_to_array(img)
9
img_array = tf.expand_dims(img_array, 0)
10
​
11
predictions = model.predict(img_array)
12
score = tf.nn.softmax(predictions[0])
13
​
14
print(
15
    "This image most likely belongs to {} with a {:.2f} percent confidence."
16
    .format(class_names[np.argmax(score)], 100 * np.max(score))
17
)
Observation:

The predicted class of the image and the confidence percentage are shown as an output.
